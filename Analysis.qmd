---
title: Global well-being and mental health in the internet age
description: | 
  Online analysis supplement to [*Global well-being and mental health in the internet age*](preprint url) (Vuorre & Przybylski, 2022)
author:
  - name: Matti Vuorre
    affiliation: University of Oxford
    affiliation-url: https://www.oii.ox.ac.uk/people/matti-vuorre/
    orcid_id: 0000-0001-5052-066X
citation:
  url: https://github.com/digital-wellbeing/global-wbmh/
date: 2022-07-06
toc: true
toc-depth: 2
toc-location: left
number-sections: true
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    html-math-method: katex
    theme: pulse
    anchor-sections: true
    self-contained: true
    self-contained-math: false
editor_options: 
  chunk_output_type: console
---

# Preface

This document is the online analysis supplement to *Global well-being and mental health in the internet age* (Vuorre & Przybylski, [submitted]()). It contains all the analyses reported in the manuscript along with supplemental analyses. The data and source code are on [GitHub](https://github.com/digital-wellbeing/global-wbmh), and an archived permanent copy of these are on [Zenodo]().

## How to reproduce our results

The analyses were conducted in R, and can be reproduced locally:

1. Clone the [github repo](https://github.com/digital-wellbeing/global-wbmh)
  - Terminal: `git clone https://github.com/digital-wellbeing/global-wbmh.git`  
      OR
  - RStudio: File -> New Project -> Version Control -> Git -> use the URL from above

2. Prepare the R environment
  - Terminal: `Rscript -e "renv::restore()"`  
      OR
  - RStudio: Click renv -> Restore Library in the Packages panel

3. Render the Quarto source file `Analysis.qmd`
 - Install [Quarto](https://quarto.org/docs/getting-started/installation.html) if you don't already have it
  - Terminal: `quarto render Analysis.qmd`  
      OR
  - RStudio: Open the file and click Render

If you encounter problems, please [open an issue](https://github.com/digital-wellbeing/global-wbmh/issues).

::: {.callout-important}
The project repo includes the GBD dataset, code to download the ITU dataset, and a synthetic mock version of the GWP dataset to enable reproducing all our computations.
**The models take several hours/days each to run**---depending on your local computing resources---and therefore the rendering process can take several days. For this reason, the build will fail after having cleaned the data. Then, run `models.R` with settings specific to your environment/cluster. Once that is done you can render the document again and it should work.
:::

# Data preparation

```{r setup, include = FALSE}
#| cache: false
#| results: hide
#| message: false
# Packages
library(readxl)
library(imputeTS)
library(naniar)
library(scales)
library(janitor)
library(haven)
library(ragg)
library(khroma)
library(labelled)
library(countrycode)
library(plotly)
library(lubridate)
library(kableExtra)
library(brms)
library(knitr)
library(here)
library(posterior)
library(bayestestR)
library(gtsummary)
library(kableExtra)
library(memoise)
library(cachem)
library(ggh4x)
library(cmdstanr)
library(ggstance)
library(tidybayes)
library(ggdist)
library(DT)
library(tidyverse)

# For tables: Show NAs as blanks
options(knitr.kable.NA = '')

# Data frame for computing over models
oo <- c(
  "Life_satisfaction", "Negative_experiences", "Positive_experiences",
  "Anxiety", "Depression", "Selfharm"
)
fits <- tibble(outcome = oo) %>%
  mutate(outcome = factor(outcome, levels = oo))

# Create directory for intermediate files
dir.create(here("data"), FALSE)

# Cache for memoised functions
cd <- cache_disk("cache", max_size = Inf)

# Plotting options
# If you've defined a custom font, we will use it in outputs
Font <- Sys.getenv("FONT")
theme_set(
  theme_linedraw(
    base_size = 9,
    base_family = Font
  ) +
    theme(
      panel.grid = element_blank(),
      strip.text = element_text(margin = margin(4, 4, 4, 4, "pt")),
      axis.text = element_text(size = rel(0.75))
    )
)

# Width of figures
W <- 9

# Output chunk options
opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE, 
  dpi = 300
)

#' Custom kableExtra formatting
#'
#' @param a table from kbl()
#'
#' @return a table
kable_custom <- function(x) {
  kable_classic(
    kable_input = x,
    html_font = "Arial",
    font_size = 13,
    position = "center",
    full_width = FALSE
  )
}
```


In this first section, we load the raw data sets and prepare them for analyses.

## Gallup World Poll

We first prepare the Gallup data. The Gallup World Poll is a proprietary dataset and includes people's responses to well-being questions across \~160 countries from 2015 to 2021. 

Note that this data set is not publicly available, and therefore not included in our repository. We provide a synthetic mock dataset for researchers who would like to reproduce our analyses but do not have access to the GWP dataset. That mock dataset is automatically loaded with a confirmation message below if this analysis is run without access to the actual GWP data.

```{r gallup-codebook}
#| echo: false
#| eval: false
# Understand variables & values
# Codes NOTE 1: yes 2: no!
read_spss(
  "data-raw/Gallup/The_Gallup_101521.sav",
  n_max = 1,
  col_select = c(
    WPID, WP1219,
    WP16,
    WP60, WP61, WP63, WP65, WP67,
    WP68, WP69, WP70, WP71, WP74
  )
) %>%
  generate_dictionary()
```

Here we import the relevant variables from Gallup's SPSS files and then

-   Clean the variable names
-   Set appropriate missing values (`NaN`, not e.g. `-99`)
-   Use appropriate coding schemes (e.g. 0: no; 1: yes)
-   Calculate scale scores (means over items)
-   Rescale outcomes to proportions (0 - 1)
-   Create age categories in line with GBD data
-   Aggregate data to means and SEs at each country * year * age * sex group
-   Drop one 13 year old as this data should only have people 15 and older
-   Exclude people older than 89

```{r gallup-clean}
# First liberate the data from the slow SPSS file and save to disk
gwp_path <- here("data-raw/Gallup/gwp-processed.rds")
if (!file.exists(gwp_path)) {
  gwp <- read_spss(
    here("data-raw/Gallup/The_Gallup_101521.sav"),
    col_select = c(
      YEAR_CALENDAR,
      COUNTRYNEW,
      WPID, WP1220, WP1219,
      WP16,
      WP60, WP61, WP63, WP65, WP67,
      WP68, WP69, WP70, WP71, WP74
    )
  )

  # Get rid of SPSS attributes
  gwp <- gwp %>%
    zap_labels() %>%
    zap_label() %>%
    zap_widths() %>%
    zap_formats()

  # Save into a good format
  write_rds(gwp, gwp_path)
} else {
  gwp <- read_rds(gwp_path)
}

# Rename and recode variables
gwp <- gwp %>%
  clean_names() %>%
  transmute(
    country = countrynew,
    year = year_calendar,
    id = wpid,
    sex = factor(wp1219, levels = c(2, 1), labels = c("Female", "Male")),
    age = wp1220,
    # Don't know, refused, and missing values
    Life_satisfaction = if_else(between(wp16, 0, 10), wp16, NaN),
    across(wp60:wp74, ~ if_else(between(., 1, 2), ., NaN)),
    # Also reverse the weird 1: yes 2: no coding here
    across(wp60:wp74, ~ 3 - .)
  )

# Scale scores note rescaling
gwp <- gwp %>%
  mutate(
    Life_satisfaction = Life_satisfaction / 10,
    Negative_experiences =
      rowMeans(select(., wp68:wp74), na.rm = TRUE) - 1,
    Positive_experiences =
      rowMeans(select(., wp60:wp67), na.rm = TRUE) - 1
  ) %>%
  select(-c(wp60:wp74))

# Categorize ages
gwp <- filter(gwp, age <= 89)
gwp <- gwp %>%
  mutate(
    age = cut(
      age,
      breaks = seq(15, 90, by = 5),
      include.lowest = TRUE,
      right = FALSE,
      labels = paste(seq(15, 85, by = 5), "to", seq(19, 89, by = 5))
    ) %>% as.character()
  )

# There is one 13 year old which resulted in a NA and we drop it
gwp <- drop_na(gwp, age)

# Some sex values are missing, drop those
gwp <- drop_na(gwp, sex)
```

Instead of working with the person-level scale scores, we aggregate the data to means and standard errors for each cell as defined by the predictors (country, year, age, sex). Because we treat outcomes as normal this greatly simplifies and speeds up the computations without affecting the results, and also makes the data format concordant with the GBD data.

```{r gallup-summarise}
gwp <- gwp %>%
  pivot_longer(
    c(
      Life_satisfaction,
      Negative_experiences,
      Positive_experiences
    ),
    names_to = "outcome", values_to = "val"
  ) %>%
  group_by(country, year, sex, age, outcome) %>%
  summarise(
    n = n(),
    se = sd(val, na.rm = TRUE) / sqrt(n),
    val = mean(val, na.rm = TRUE)
  ) %>%
  ungroup()

# Note that if there is no response variance, SE will be zero, and if there is only 1 response, SE will be NA. Here we fix those by assigning them the maximum SE (0.5)
gwp <- gwp %>% 
  mutate(
    se = if_else(is.na(se) | se == 0, 0.5, se)
  )
```

We have multiple data sets with information on countries. Unfortunately, they can all use idiosyncratic naming conventions for the countries, and we therefore harmonise the names in each dataset to the same standard values. We use the short English names from the UNICODE CLDR project as provided by `countrycode::countryname()`. We do that here for the GWP data, and check that all countries receive a unique name.

```{r gwp-harmonise-countries}
#| label: tbl-gwp-countries
#| tbl-cap: "Countries whose original GWP name is different to the harmonised one, or whose harmonised name is missing and therefore the original name is retained."

# Check what names were changed and if any don't have harmonised counterpart
gwp %>%
  distinct(country) %>%
  arrange(country) %>%
  mutate(
    country_harmonised = countryname(country, destination = "cldr.short.en")
  ) %>%
  filter(country != country_harmonised | is.na(country_harmonised)) %>%
  kbl() %>%
  kable_custom()

# We can see that this would result in North Cyprus being lumped with Cyprus so we need to replace the name before harmonising

# Harmonise old names and replace only if harmonised name found
gwp <- gwp %>%
  mutate(
    country_harmonised = countryname(
      country,
      destination = "cldr.short.en"
    )
  ) %>%
  # This picks the harmonised name if exists, otherwise original name
  # Prevent north cyprus from becoming cyprus
  mutate(
    country_harmonised = ifelse(
      country == "Northern Cyprus",
      "Northern Cyprus",
      country_harmonised
    )
  ) %>%
  mutate(country = coalesce(country_harmonised, country)) %>%
  select(-country_harmonised)
```

### Synthetic GWP data

If the real GWP data was not available above, here we load the synthetic dataset (having first created it).

```{r mock-gwp-data}
gwp_mock_path <- here("data-raw/gwp-MOCK.rds")
if (dir.exists(here("data-raw/Gallup")) & !file.exists(gwp_mock_path)) {
  library(synthpop)
  gwp_syn <- gwp %>% 
    drop_na()
  gwp_syn <- syn(
    gwp_syn, 
    method = c("", "", "", "", "", "", "normrank", "normrank"),
    maxfaclevels = 168
  )
  gwp_syn <- tibble(gwp_syn$syn)
  saveRDS(gwp_syn, gwp_mock_path)
}

# Load if actual data doesn't exist
if (!dir.exists(here("data-raw/Gallup"))) {
  cat("Using synthetic GWP data.")
  gwp <- read_rds(gwp_mock_path)
} else {
  cat("Using actual GWP data.")
}
```

## Global Burden of Disease

The Global Burden of Disease dataset consists of mental health indicators across \~200 countries from 2000 to 2019.

We downloaded the Global Burden of Disease data from <http://ghdx.healthdata.org/gbd-results-tool> on 2021-11-02 to `data-raw/GBD/`. We include those files in our repository as [permitted by the license](http://ghdx.healthdata.org/). They are inside downloaded `.zip` files, of which there may be more than one (depending on the download size). Here, we load those tables to R.

```{r gbd-data-load}
# Load data files and merge to one table
gbd <- list.files(
  here("data-raw/GBD/"),
  pattern = ".zip", full.names = TRUE, recursive = TRUE
) %>%
  read_csv()
```

We then clean the GBD data and

-   Clean the variable names
-   Remove the metric and measure variables; we focus on prevalence rate
    -   prevalence: Total number of cases
    -   rate: Total cases per 100,000 population
-   Convert outcome rates to proportions (0 - 1)
-   Clean cause names and rename to outcome to harmonise with GWP
-   Convert GBD estimated rate CI limits to an approximate standard error

```{r gbd-data-clean}
# First step is to clean names
gbd <- clean_names(gbd)

# Confirm that correct measures were downloaded:
# distinct(gbd, measure, metric)
gbd <- select(gbd, -metric, -measure)
gbd <- gbd %>%
  mutate(across(c(val, upper, lower), ~ .x / 100000))

# Clean cause names
# distinct(gbd, cause)
gbd <- gbd %>%
  mutate(
    cause = case_when(
      cause == "Anxiety disorders" ~ "Anxiety",
      cause == "Depressive disorders" ~ "Depression",
      cause == "Self-harm" ~ "Selfharm"
    )
  )

# Harmonise variable names with other datasets
gbd <- gbd %>%
  rename(country = location, outcome = cause)

# The outcomes values are model predictions and come with lower and upper CI limits (2.5 and 97.5 %iles of their posterior distributions). We convert those to normal approximate standard errors.
gbd <- gbd %>%
  mutate(se = (upper - lower) / (1.96 * 2)) %>%
  select(-c(upper, lower))
```

Then we need to harmonise to country names (always somewhat idiosyncratic) to a common metric. There is a harmonised name for each GBD country, and 27 country names are harmonised.

```{r gbd-harmonise-countries}
#| label: tbl-gbd-countries
#| tbl-cap: "Countries whose original GBD name is different to the harmonised one, or whose harmonised name is missing"

# Check what names were changed and if any don't have harmonised counterpart
gbd %>%
  distinct(country) %>%
  arrange(country) %>%
  mutate(
    country_harmonised = countryname(country, destination = "cldr.short.en")
  ) %>%
  filter(country != country_harmonised | is.na(country_harmonised)) %>%
  kbl() %>%
  kable_custom()
# Harmonise old names and replace only if harmonised name found
gbd <- gbd %>%
  mutate(
    country_harmonised = countryname(
      country,
      destination = "cldr.short.en"
    )
  ) %>%
  # This picks the harmonised name if exists, otherwise original name
  mutate(country = coalesce(country_harmonised, country)) %>%
  select(-country_harmonised)
```

## International Telecommunication Union

After processing the outcome tables above, we move on to the internet adoption metrics from the ITU. The International Telecommunications Union dataset has internet adoption metrics across \~200 countries from 2000 to 2020. 

We downloaded the data files from the ITU website. New versions may be posted subsequent to this, in which case the URLs could be changed here to update the analyses with latest data.

After downloading, we cleaned the ITU data to a shape concordant with the outcome data, and converted the values to proportions (0 - 1). Then we harmonised the country names as above, and dropped countries that don't exist in outcomes.

```{r itu-process}
#| results: hold
#| label: tbl-itu-0
#| tbl-cap: "Countries whose original ITU name is different to the harmonised one, or whose harmonised name is missing"

# Download data sets from the ITU website if not yet downloaded
if (!file.exists(here("data-raw/ITU/PercentIndividualsUsingInternet.xlsx"))) {
  dir.create("data-raw/ITU")
  download.file(
    "https://www.itu.int/en/ITU-D/Statistics/Documents/statistics/2021/December/PercentIndividualsUsingInternet.xlsx",
    here("data-raw/ITU/PercentIndividualsUsingInternet.xlsx")
  )
  download.file(
    "https://www.itu.int/en/ITU-D/Statistics/Documents/statistics/2021/December/FixedBroadbandSubscriptions_2000-2020.xlsx",
    here("data-raw/ITU/FixedBroadbandSubscriptions_2000-2020.xlsx")
  )
  download.file(
    "https://www.itu.int/en/ITU-D/Statistics/Documents/statistics/2021/December/MobileBroadbandSubscriptions_2007-2020.xlsx",
    here("data-raw/ITU/MobileBroadbandSubscriptions_2007-2020.xlsx")
  )
}

# Read and reshape fixed broadband data
itu_fixed <- read_xlsx(
  here("data-raw/ITU/FixedBroadbandSubscriptions_2000-2020.xlsx"),
  sheet = 2,
  col_types = "text"
) %>%
  pivot_longer(-c(Indicator, Country)) %>%
  separate(name, into = c("year", "variable")) %>%
  pivot_wider(names_from = variable) %>%
  mutate(fixed = as.numeric(value)) %>%
  select(country = Country, year, fixed)

# Read and reshape mobile broadband data
itu_mobile <- read_xlsx(
  here("data-raw/ITU/MobileBroadbandSubscriptions_2007-2020.xlsx"),
  sheet = 2,
  col_types = "text"
) %>%
  pivot_longer(-c(Indicator, Country)) %>%
  separate(name, into = c("year", "variable")) %>%
  pivot_wider(names_from = variable) %>%
  mutate(mobile = as.numeric(value)) %>%
  select(country = Country, year, mobile)

# Percent using internet
itu_percent <- read_xlsx(
  here("data-raw/ITU/PercentIndividualsUsingInternet.xlsx"),
  col_types = "text"
) %>%
  pivot_longer(-c(Indicator, Country)) %>%
  separate(name, into = c("year", "variable")) %>%
  pivot_wider(names_from = variable) %>%
  mutate(internet = as.numeric(value)) %>%
  select(country = Country, year, internet)

# Merge to one table in wide format (oldest on left to include all years)
itu <- reduce(list(itu_percent, itu_fixed, itu_mobile), left_join)

# values (1-100) to proportions (0-1) and years to numbers
itu <- itu %>%
  mutate(across(c(internet, fixed, mobile), function(x) x / 100)) %>%
  mutate(year = as.numeric(year))

# Drop tables
rm(itu_fixed, itu_mobile, itu_percent)

# Harmonise countries
# Check what names were changed and if any don't have harmonised counterpart
itu %>%
  distinct(country) %>%
  arrange(country) %>%
  mutate(
    country_harmonised = countryname(country, destination = "cldr.short.en")
  ) %>%
  filter(country != country_harmonised | is.na(country_harmonised)) %>%
  kbl() %>%
  kable_custom()

# Harmonise old names and replace only if harmonised name found
itu <- itu %>%
  mutate(
    country_harmonised = countryname(
      country,
      destination = "cldr.short.en"
    )
  ) %>%
  # This picks the harmonised name if exists, otherwise original name
  mutate(country = coalesce(country_harmonised, country)) %>%
  select(-country_harmonised)

# Drop countries that don't exist in outcomes
itu <- itu %>%
  filter(
    country %in% unique(gbd$country) | country %in% unique(gwp$country)
  )

# Drop countries that don't have any actual internet data
itu_zeros <- itu %>%
  group_by(country) %>%
  summarise(s = sum(internet, na.rm = TRUE)) %>%
  filter(s == 0)

itu <- itu %>%
  filter(!(country %in% itu_zeros$country))
rm(itu_zeros)
```

### Missingness

We then impute missing intermediate values to the internet adoption timeseries. We use linear interpolation to fill missing intermediate values, but do not extrapolate before or after the first and last values of each country.

```{r itu-impute}
#| results: hold
#| label: tbl-itu-impute
#| tbl-cap: "Summary of ITU data imputation"

# Reshape to easily impute all variables
itu_long <- itu %>%
  pivot_longer(c(internet, mobile))

# Impute by country and variable
itu_long <- itu_long %>%
  group_by(country, name) %>%
  # Find first and last year with this variable in each country
  # so as to prevent extrapolation
  mutate(
    min_year = year[min(which(!is.na(value)))],
    max_year = year[max(which(!is.na(value)))]
  ) %>%
  # Interpolate values linearly
  # use possibly() to avoid errors for countries with no data
  mutate(
    across(
      value,
      .fns = list(
        i = possibly(
          function(x) na_interpolation(x, maxgap = Inf),
          otherwise = NaN
        )
      )
    )
  ) %>%
  # Take out projected (non-intermediate) imputed values
  mutate(
    value_i = if_else(
      between(year, unique(min_year), unique(max_year)),
      value_i,
      NaN
    )
  ) %>%
  ungroup() %>%
  select(-min_year, -max_year)

# Summary
itu_long %>%
  group_by(name) %>%
  mutate(min_year = if_else(name == "mobile", 2007, 2000)) %>%
  summarise(
    n_miss_orig = sum(is.na(value) & year >= min_year),
    p_miss_orig = percent(n_miss_orig / sum(year >= min_year), .1),
    n_miss_imp = sum(is.na(value_i) & year >= min_year),
    p_miss_imp = percent(n_miss_imp / sum(year >= min_year), .1),
    imputed = n_miss_orig - n_miss_imp
  ) %>%
  kbl() %>%
  kable_custom()


# We then replace the original values with the imputed ones
itu <- itu_long %>%
  # Pick non-missing value, from original and imputed
  mutate(value = coalesce(value, value_i)) %>%
  select(-value_i) %>%
  pivot_wider()
rm(itu_long)
```

## Regions

We also include a `region` variable in the data ("Continent as defined in the World Bank Development Indicators") to allow grouping countries in figures and outputs.

```{r}
#| label: tbl-regions
#| tbl-cap: "Countries in each region."

regions <- bind_rows(
  distinct(itu, country),
  distinct(gbd, country),
  distinct(gwp, country)
) %>% 
  distinct() %>% 
  mutate(
    region = countrycode(
      country, 
      origin = "country.name", 
      destination = "continent"
    )
  )

# Manually fix two countries
regions <- regions %>% 
  mutate(
    region = if_else(country == "Kosovo", "Europe", region),
    region = if_else(country == "Nagorno Karabakh", "Asia", region)
    )

# Add regions to data tables
itu <- left_join(itu, regions)
gbd <- left_join(gbd, regions)
gwp <- left_join(gwp, regions)

regions %>% 
  group_by(region) %>%
  summarise(
    N = n(),
    Countries = str_c(country, collapse = ", ")
  ) %>% 
  kbl() %>% 
  kable_custom()
```

## Prepare model data

We then further wrangle this data in preparation for the models:

-   Combine all predictors into one table
-   Create lagged and centered predictors
-   Join outcome and predictor tables
-   Center year on 2010 (roughly in the middle) and divide by ten ("effect" of decades)

```{r model-prepare-data}
# Create lagged predictors
itu <- itu %>% 
  arrange(region, country, year) %>%
  group_by(region, country) %>%
  mutate(
    across(
      c(internet, mobile),
      list(`1` = ~ lag(., 1)),
      .names = "{str_sub(.col, 1, 1)}{.fn}"
    )
  ) %>% 
  ungroup()

# Centering predictors within and between countries
itu <- itu %>%
  # Grand mean centering so both components are zero-centered
  mutate(across(c(i1, m1), ~. - mean(., na.rm = TRUE))) %>% 
  group_by(country) %>%
  mutate(
    # Between
    i1_cb = mean(i1, na.rm = TRUE),
    m1_cb = mean(m1, na.rm = TRUE),
    # Within
    i1_cw = i1 - i1_cb, 
    m1_cw = m1 - m1_cb
  ) %>% 
  ungroup()

# Stack outcome data
dat <- bind_rows(gwp, gbd)

# Merge predictors to outcomes
dat <- left_join(dat, itu)

# Center and scale year
dat <- dat %>%
  mutate(year = (year - 2010) / 10)

# Identify values to use in MV models
# Indicate unique ITU values for MV model
dat <- dat %>%
  # Unique values exist for each country-year, but must be given for each
  # outcome
  group_by(region, country, year, outcome) %>%
  mutate(itu = 1:n() == 1, itum = 1:n() == 1) %>%
  ungroup()

# This prevents a glitch where rows where ITU==TRUE but internet is missing would not be included in model of val
dat <- dat %>% 
  mutate(
    itu = if_else(itu & is.na(internet), FALSE, itu),
    itum = if_else(itum & is.na(mobile), FALSE, itum)
    )

# Ensure that categorical variables are factors
dat <- dat %>%
  mutate(across(c(sex, age), factor))

# Model outcomes on the percentage (0-100) scale. Note internet is still 0-1
dat <- dat %>%
  mutate(across(c(val, se), ~ . * 100))

# Arrange on outcome
oo <- c(
  "Life_satisfaction", "Negative_experiences", "Positive_experiences",
  "Anxiety", "Depression", "Selfharm"
)
dat <- dat %>%
  mutate(outcome = factor(outcome, levels = oo)) %>% 
  arrange(outcome, country, region, year)

# Selfharm models don't converge with SEs
dat <- dat %>% 
  mutate(se = if_else(outcome == "Selfharm", 0, se))
```

# Data description

Here are some descriptives of the datasets.

## Sample summaries

Number of countries/years/people per outcome

```{r}
#| label: tbl-data-n
#| tbl-cap: "Numbers of respondents and years per outcome and region."

bind_rows(
  "GWP" = gwp,
  "GBD" = gbd,
  .id = "Dataset"
) %>% 
  mutate(Dataset = fct_inorder(Dataset)) %>% 
  group_by(Dataset, region, outcome, country) %>% 
  summarise(
    n_n = sum(n),
    n_year = length(unique(year))
  ) %>% 
  summarise(
    N = sum(n_n), 
    Years = mean(n_year)
  ) %>%
  summarise(
    N_sum = round(mean(N)), 
    Years_mean = round(mean(Years))
  ) %>%
  kbl() %>% 
  kable_custom()
tmp <- gwp %>% 
  group_by(outcome) %>% 
  summarise(n=sum(n)) %>% 
  summarise(N=comma(median(n))) %>% 
  pull(N)
```

The median number of respondents per Gallup well-being outcome was `r tmp`, and the unique age bins in the GWP data were `r unique(gwp$age)`. 

We then examined the total number of observations (cell means and SEs) per outcome

```{r}
#| label: tbl-obs-per-outcome
#| tbl-cap: "Numbers of observations per outcome."
bind_rows(
  "GWP" = gwp,
  "GBD" = gbd,
  .id = "Dataset"
) %>% 
  mutate(Dataset = fct_inorder(Dataset)) %>% 
  count(Dataset, outcome) %>% 
  bind_rows(
    itu %>% 
      select(internet, mobile) %>% 
      pivot_longer(everything(), names_to = "outcome") %>% 
      drop_na(value) %>% 
      count(outcome) %>% 
      mutate(Dataset = "ITU")
  ) %>% 
  kbl() %>% 
  kable_custom()
```

And noted that the number of Gallup respondents decreases with age:

```{r}
#| label: tbl-gallup-age
#| tbl-cap: "Quantiles of numbers of observations per age group"

gwp %>% 
  # filter(age %in% c("15 to 19", "50 to 54", "85 to 89")) %>% 
  group_by(age) %>% 
  summarise(
    n = quantile(n, probs = c(.1, .5, .9)),
    quantile = c("10%", "50%", "90%")
  ) %>% 
  pivot_wider(names_from = quantile, values_from = n) %>% 
  kbl() %>% 
  kable_custom()
```

## ITU summary table

Here is a table summarising the ITU internet predictor over time to region average percentage and number of coutries.

```{r itu-summary-table}
#| tbl-cap: "Mean internet user percentages across regions and time."
#| label: tbl-itusummary

itu %>%
  select(region, country, year, internet) %>%
  mutate(internet = internet * 100) %>%
  pivot_wider(names_from = year, values_from = internet) %>%
  select(-country) %>%
  tbl_summary(
    by = region,
    missing = "no",
    statistic = list(all_continuous() ~ "{mean}% ({N_nonmiss})")
  ) %>%
  add_overall(col_label = "Total") %>%
  as_kable_extra() %>%
  row_spec(0, bold = TRUE) %>%
  kable_custom()
```

## Figure 1

This figure shows the timeseries of each country's key variables (aggregated over sex and age for outcomes).

:::{.panel-tabset .column-body-outset-right}

### Static

```{r figure-1, message = FALSE}
#| fig-width: 10
#| fig-height: 8
#| fig-cap: "Trends in all variables across regions"
#| label: fig-1

# Stack data sets
# Assume all outcomes are proportions (0-1)
# Draw small multiples figure, outcomes x regions
d <- bind_rows(
  gwp,
  gbd
)

itu <- itu %>%
  select(
    region, country, year,
    Internet = internet,
    Mobile = mobile,
  ) %>%
  # Truncate mobile percentages for figure
  mutate(Mobile = if_else(Mobile > 1.5, NaN, Mobile)) %>%
  pivot_longer(
    c(Internet, Mobile),
    names_to = "outcome", values_to = "val"
  )

p1 <- bind_rows(itu, d) %>%
  mutate(outcome = fct_inorder(str_replace(outcome, "_", "_"))) %>%
  group_by(region, country, year, outcome) %>%
  summarise(mean = mean(val, na.rm = TRUE)) %>%
  ungroup() %>% 
  mutate(outcome = factor(outcome, levels = c("Internet", "Mobile", oo))) %>% 
  mutate(year = make_date(year)) %>%
  ggplot(aes(year, mean)) +
  scale_y_continuous(
    "Value (%)",
    # Display percentages
    labels = ~. * 100,
    breaks = pretty_breaks(),
    expand = expansion(.05)
  ) +
  scale_x_date(
    "Year",
    breaks = make_date(c(2005, 2010, 2015, 2020)),
    date_labels = "'%y",
    expand = expansion(.02)
  ) +
  geom_line(
    aes(
      group = interaction(country, outcome), 
      text = str_glue("{country} {year(year)} ({percent(mean)})")
    ),
    size = .25, col = "gray20"
  ) +
  facet_grid(
    outcome ~ region,
    scales = "free_y",
    labeller = as_labeller(function(x) str_replace(x, "_", "\n"))
  ) +
  theme(
    # aspect.ratio = 1,
    legend.position = "none",
    panel.spacing.x = unit(4, "pt"),
    panel.spacing.y = unit(6, "pt")
  )
p1
```

### Interactive

```{r}
#| fig-width: 10
#| fig-height: 8
#| fig-cap: "Trends in all variables across regions (interactive)"
#| label: fig-trends-regions-interactive

gp <- ggplotly(p1, tooltip = "text")
gp %>% 
  layout(
    margin = list(t = 30, r = 20, b = 40, l = 30)
    )
```

:::


# Models

Here we discuss the models. The code for estimating these models is in `models.R`. We submit that to our local HPC with SLURM scheduler with `submit.sh`. 

## Model 1

Model 1 is a two-level probabilistic multivariate meta-regression model that simultaneously predicts the health (e.g. Life satisfaction) and internet variables (e.g. internet adoption) from time, sex, and time by sex interactions. These regression coefficients are modelled as multivariate normal distributed over countries, age groups, and the country by age groups. The latter clustering was dropped from the mental health variables because of nonconvergence due to lack of variance in data ($\Sigma^\text{age:country} = 0$). The country-level covariance matrices are shared across the two outcomes so we can assess correlations between countries' intercepts and slopes over the two outcomes.

Because we are modelling meta-analytic estimates (GBD) and aggregated scores (GWP), we also incorporate the aggregates' standard errors in the model to weight the observations on their corresponding uncertainties. Due to nonconvergence, we set the SEs to zero for self-harm ($v = 0$).

$$
\begin{align}
y_i &\sim \text{Normal}(\mu^y_i, \sigma^{y2}v_i) \\
x_i &\sim \text{Normal}(\mu^x_i, \sigma^{x2}) \\ 
\mu^y_i &= \alpha^{y}_{0} + \beta^{y}_{0\text{country}[i]} + \gamma^{y}_{0\text{age}[i]} + \delta^{y}_{0\text{age:country}[i]} + \\ 
&(\alpha^{y}_{1} + \beta^{y}_{1\text{country}[i]} + \gamma^{y}_{1\text{age}[i]} + \delta^{y}_{1\text{age:country}[i]})\text{Time}_i + \\
&(\alpha^{y}_{2} + \beta^{y}_{2\text{country}[i]} + \gamma^{y}_{2\text{age}[i]} + \delta^{y}_{2\text{age:country}[i]})\text{Sex}_i + \\
&(\alpha^{y}_{3} + \beta^{y}_{3\text{country}[i]} + \gamma^{y}_{3\text{age}[i]} + \delta^{y}_{3\text{age:country}[i]})\text{Sex}_i \times \text{Time}_i \\
\mu^x_i &= \alpha^{x}_{0} + \beta^{x}_{0\text{country}[i]} + (\alpha^x_{1} + \beta^{x}_{1\text{country}[i]})\text{Time}_i \\
\pmb{\beta} &\sim MVN(\pmb{0}, \Sigma^\text{country}) \\
\pmb{\gamma} &\sim MVN(\pmb{0}, \Sigma^\text{age}) \\
\pmb{\delta} &\sim MVN(\pmb{0}, \Sigma^\text{age:country})
\end{align}
$$ {#eq-model1}

## Model 2

Model 2 was similar to Model 1, but the internet outcome was dropped, and the regression equation for outcomes expanded to include the lagged internet predictor and its two-way interactions

$$
\begin{align}
y_i &\sim \text{Normal}(\mu_i, \sigma^{y2}v_i) \\
\mu_i &= \alpha_{0} + \beta_{0\text{country}[i]} + \gamma_{0\text{age}[i]} + \delta_{0\text{age:country}[i]} + \\ 
&(\alpha_{1} + \beta_{1\text{country}[i]} + \gamma_{1\text{age}[i]} + \delta_{1\text{age:country}[i]})\text{Time}_i + \\
&(\alpha_{2} + \beta_{2\text{country}[i]} + \gamma_{2\text{age}[i]} + \delta_{2\text{age:country}[i]})\text{Sex}_i + \\
&(\alpha_{3} + \beta_{3\text{country}[i]} + \gamma_{3\text{age}[i]} + \delta_{3\text{age:country}[i]})\text{Sex}_i \times \text{Time}_i + \\
&(\alpha_{4} + \beta_{4\text{country}[i]} + \gamma_{4\text{age}[i]} + \delta_{4\text{age:country}[i]})\text{Internet}^{\text{t-1}}_i + \\
&(\alpha_{5} + \beta_{5\text{country}[i]} + \gamma_{5\text{age}[i]} + \delta_{5\text{age:country}[i]})\text{Sex}_i \times \text{Internet}^{\text{t-1}}_i + \\
&\alpha_6 \text{Internet}^{\text{t-1[CB]}}_i
\\
\pmb{\beta} &\sim MVN(\pmb{0}, \Sigma^\text{country}) \\
\pmb{\gamma} &\sim MVN(\pmb{0}, \Sigma^\text{age}) \\
\pmb{\delta} &\sim MVN(\pmb{0}, \Sigma^\text{age:country})
\end{align}
$$ {#eq-model2}

Where $\text{Internet}^{\text{t-1}}_i$ is the within-country centered 1-year-lagged internet user proportion (or mobile subscriptions). $\text{Internet}^{\text{t-1[CB]}}_i$ are the between-country centered values.

## Model checking and parameters

Next, we display the population-level parameters of each model, along with their numerical HMC diagnostics (ESS and Rhat values).

```{r}
# Create a table of outcomes in appropriate order, so can map functions over this
oo <- c(
  "Life_satisfaction", "Negative_experiences", "Positive_experiences",
  "Anxiety", "Depression", "Selfharm"
)
fits <- tibble(outcome = oo) %>% 
  mutate(outcome = fct_inorder(outcome))

# The files are huge and unwieldy to work with. We create smaller files for each by dropping samples of the age:country interaction random effects. It is then easier to post-process the models.
dir.create("models-small", FALSE)

replace_chain <- function(x) {
  x %>%
    select(
      -starts_with("r_1"),
      -starts_with("r_2"),
      -starts_with("r_3"),
      -starts_with("z_1"),
      -starts_with("z_2"),
      -starts_with("z_3"),
      -starts_with("r_age:country"),
      -starts_with("L_")
    )
}

replace_chain_all <- function(x) {
  if (!file.exists(str_glue("models-small/{x}"))) {
    message(x)
    out <- readRDS(str_glue("models/{x}"))
    out$fit@sim$samples <- map(1:4, ~replace_chain(out$fit@sim$samples[[.]]))
    saveRDS(out, str_glue("models-small/{x}"), compress = FALSE)
  }
}

walk(
  list.files("models"),
  ~replace_chain_all(.x)
)
```

### Diagnostics

Here we take a look at model convergence using numerical summaries (rhat) and scatterplot matrices of posterior draws.

```{r model-diagnostics}
#| label: tbl-rhats
#| tbl-cap: "Numerical convergence diagnostics for parameters with suspicious rhats."

parameter_summary <- function(outcome, model) {
  path <- str_glue("models-small/brm-{outcome}-{model}.rds")
  as_draws_df(
    readRDS(path), 
    variable = c("^b_", "^sd_", "^cor_"), 
    regex = TRUE
  ) %>%
    summarise_draws(
      mean, sd,
      samples = length, 
      default_convergence_measures()
    )
}
parameter_summary <- memoise(parameter_summary, cache = cd)

convergence_summary <- tibble(
  path = list.files("models-small/", pattern = ".rds$", full.names = FALSE)
) %>% 
  separate(path, c("brm", "outcome", "model"), sep = "-") %>% 
  select(-brm) %>% 
  mutate(outcome = factor(outcome, levels = oo)) %>% 
  arrange(outcome) %>% 
  mutate(model = str_extract(model, "[0-9]")) %>% 
  mutate(parameters = map2(outcome, model, ~parameter_summary(.x, .y))) %>% 
  unnest(parameters)

convergence_summary %>% 
  filter(abs(rhat) > 1.03) %>% 
  arrange(outcome, model, variable) %>% 
  kbl(digits = c(rep(2, 7), 0, 0)) %>% 
  kable_custom()
```

For each model, we then draw a scatterplot matrix of parameters with worst rhats. Browse these figures at `figures/diagnostics/`.

```{r ppcheck-scatter}
#| eval: !expr file.exists("figures/diagnostics/brm-Anxiety-1.png")*-1

# Function to draw a pairsplot of variables `v` of model in `path` 
dir.create(here("figures/diagnostics"), FALSE)
pairsplot <- function(filename, v) {
  out_path <- here(str_glue("figures/diagnostics/{basename(filename)}.png"))
  if (!file.exists(out_path)) {
    x <- readRDS(here(filename))
    agg_png(
      out_path,
      width = W*1.2, height = W*1.2, units = "in", res = 160
    )
    pairs(x$fit, pars = v)
    title(sub = out_path)
    dev.off()
  }
}

# Draw pairsplot for 10 worst rhats per model
convergence_summary %>% 
  group_by(outcome, model) %>% 
  arrange(outcome, model, desc(rhat)) %>% 
  slice(1:10) %>% 
  ungroup() %>% 
  group_by(outcome, model) %>% 
  summarise(v = list(variable)) %>% 
  mutate(path = str_glue("models-small/brm-{outcome}-{model}.rds")) %>% 
  ungroup() %>% 
  {walk2(.$path, .$v, ~pairsplot(.x, .y))}
```

We determined that, although a handful of parameters had slightly suspicious rhats, the chains had converged well and the HMC outputs are reliable, because the scatterplots checked out well.

# Results

The main results of the study are calculated and presented here.

```{r}
# Before drawing any plots, we need to extract/compute the relevant parameters from the models. We do that here: For each outcome, we get the countries' average changes over time (in the outcome, internet, and mobile) and associations with internet and mobile
country_post_fun <- function(outcome) {
  
  fit1 <- readRDS(str_glue("models-small/brm-{outcome}-1.rds"))
  fit2 <- readRDS(str_glue("models-small/brm-{outcome}-2.rds"))
  fit3 <- readRDS(str_glue("models-small/brm-{outcome}-3.rds"))
  fit4 <- readRDS(str_glue("models-small/brm-{outcome}-4.rds"))
  
  out_outcome_year <- spread_draws(
    fit1,
    b_val_year, r_country__val[country, x] | country
  ) %>% 
    filter(x == "year") %>% 
    ungroup() %>% 
    rename(World = b_val_year) %>% 
    mutate(y = "outcome")
  
  out_internet_year <- spread_draws(
    fit1,
    b_internet_year, r_country__internet[country, x] | country
  ) %>% 
    filter(x == "year") %>% 
    ungroup() %>% 
    rename(World = b_internet_year) %>% 
    mutate(y = "Internet")
  
  out_mobile_year <- spread_draws(
    fit2,
    b_mobile_year, r_country__mobile[country, x] | country
  ) %>% 
    filter(x == "year") %>% 
    ungroup() %>% 
    rename(World = b_mobile_year) %>% 
    mutate(y = "Mobile")
  
  out_outcome_internet <- spread_draws(
    fit3,
    b_i1_cw, r_country[country, x] | country
  ) %>% 
    filter(x == "i1_cw") %>% 
    ungroup() %>% 
    rename(World = b_i1_cw) %>% 
    mutate(y = "outcome")
  
  out_outcome_mobile <- spread_draws(
    fit4,
    b_m1_cw, r_country[country, x] | country
  ) %>% 
    filter(x == "m1_cw") %>% 
    ungroup() %>% 
    rename(World = b_m1_cw) %>% 
    mutate(y = "outcome")
  
  out <- bind_rows(
    out_outcome_year, 
    out_internet_year,
    out_mobile_year,
    out_outcome_internet,
    out_outcome_mobile
  ) %>%  
    select(x, y, everything(), -c(.chain, .iteration, .draw)) %>% 
    # Add average to all but itself
    mutate(across(-c(x, y, World), ~. + World)) %>% 
    rename_with(~str_replace_all(., "\\.(?!\\.)", " ")) %>% 
    mutate(
      x = factor(
        x, 
        levels = c("year", "i1_cw", "m1_cw"),
        labels = c("Time", "Internet", "Mobile")
      )
    )
  
  out 
}
country_post_fun <- memoise(country_post_fun, cache = cd)
country_post <- fits %>% 
  mutate(
    samples = map(outcome, country_post_fun),
    summary = map(
      samples, 
      ~.x %>% 
        group_by(x, y) %>% 
        summarise_draws(
          mean, 
          ~quantile2(., probs = c(.025, .975)),
          pd = ~pd(as.numeric(.))
        ) %>% 
        rename(country = variable)
    )
  )

# Create scaling factors for decade-equivalent associations
# There is a different scaling factor for each outcome,
# even though they used the same internet/mobile values. That's because 
# different countries were included by outcome, and its then most faithful
# to the data to keep these separate by outcome.
# Get each country's Time -> Predictor parameters
scaling_factors <- country_post %>% 
  select(outcome, summary) %>% 
  unnest(summary) %>% 
  filter(x == "Time", y != "outcome") %>% 
  select(outcome, y, country, scaling = mean)

# Find changes per decade per outcome for ROPE limits
change_factors <- country_post %>% 
  select(outcome, summary) %>% 
  unnest(summary) %>% 
  filter(x == "Time", y == "outcome") %>% 
  select(outcome, country, change = mean)

# Calculate sign and ROPE tests for time/internet/mobile -> outcomes
summarise_all <- function(
    posterior = country_post,
    scaling_factors = scaling_factors,
    change_factors = change_factors
) {
  
  # Posterior samples of all effects on outcomes
  tab <- posterior %>% 
    select(outcome, samples) %>% 
    unnest(samples) %>% 
    filter(y == "outcome") %>%
    select(-y) %>%
    pivot_longer(-c(outcome, x), names_to = "country") %>% 
    # Drop NAs because some country-outcome combinations don't exist
    drop_na(value) %>% 
    # We will have raw and scaled results
    rename(raw = value)
  
  # Scale internet and mobile to decade-equivalents
  tab <- tab %>% 
    left_join(
      scaling_factors %>% 
        rename(x = y)
    ) %>% 
    mutate(scaling = if_else(x == "Time", 1, scaling)) %>% 
    mutate(scaled = raw * scaling) %>% 
    select(-scaling)
  
  # Change associations with internet and mobile to units of 10% not 100%
  tab <- tab %>% 
    mutate(
      raw = if_else(x == "Time", raw, raw / 10)
    )
  
  # Calculate the raw and scaled results for each outcome and predictor
  tab <- tab %>% 
    nest(data = c(raw, scaled)) %>% 
    left_join(change_factors) %>% 
    mutate(
      out = map2(
        data, change,
        ~describe_posterior(
          .x, 
          centrality = "mean", 
          ci_method = "eti",
          rope_range = c(-.y, .y), 
          rope_ci = 1
        )
      )
    ) %>% 
    transmute(
      Predictor = fct_inorder(x), 
      Outcome = fct_inorder(str_replace(outcome, "_", " ")),
      country, 
      out
    ) %>% 
    arrange(Predictor, Outcome) %>% 
    unnest(out) %>% 
    # Clean the names a bit
    rename(
      rope = ROPE_Percentage,
      mean = Mean,
      low = CI_low,
      high = CI_high
    ) %>% 
    # Put raw and scaled results on one row
    pivot_wider(
      names_from = Parameter,
      values_from = c(mean, low, high, pd, rope),
      names_glue = "{Parameter}_{.value}"
    ) %>% 
    # Leave symmetric ROPE limit in table
    mutate(rope_limit = abs(ROPE_low)) %>% 
    select(
      Predictor, 
      Outcome, 
      Country = country, 
      starts_with("raw_"), 
      starts_with("scaled_"),
      # pd is common to raw and scaled values, rope only applies to scaled values
      pd = raw_pd,
      rope = scaled_rope,
      rope_limit,
      -raw_rope, 
      -scaled_pd
    )
  
  # Calculate decision based on pd and rope
  tab <- tab %>% 
    mutate(
      d_nhst = case_when(
        pd < 0.975 ~ "Null",
        pd >= 0.975 & sign(raw_mean) == -1 ~ "Negative",
        pd >= 0.975 & sign(raw_mean) == 1 ~ "Positive"
      ),
      d_rope = case_when(
        scaled_high < rope_limit*-1 ~ "Negative",
        scaled_low > rope_limit ~ "Positive",
        rope > 0.95 ~ "Null",
        TRUE ~ "Inconclusive"
      )
    )
  
  # Return object
  tab
}
summarise_all <- memoise(summarise_all, cache = cd)
tab <- summarise_all(country_post, scaling_factors, change_factors)
```

## Average parameters

### Table 1

This table shows the raw coefficients of the outcome variables' associations with Time (change over time), Internet, and Mobile. These are for the average country. For those associations, it displays the posterior probability of direction of the association. 

It also shows the decade-equivalent associations between Internet and Mobile and each outcome. For those, it displays the posterior probability that the association is smaller than the simple association with time.

```{r}
#| label: tbl-average
#| tbl-cap: "Average parameter estimates"

# Function to replace extreme percentages
percent2 <- function(x, accuracy = .1) {
  x <- percent(x, accuracy = accuracy)
  x <- if_else(x == "100.0%", ">99.9%", x)
  x <- if_else(x == "0.0%", "<0.1%", x)
  x
}

# Function to align positive and negative values
number2 <- function(x, accuracy = .001, pad = FALSE) {
  out <- number(
    x,
    accuracy = accuracy,
    style_negative = "minus"
    )
  
  # Justify on decimal point
  if (pad) {
    # Add white space to start of non-negative numbers
    out[str_starts(out, "[0-9]")] <- str_c(
      "<code style='background:white'> </code>", 
      out[str_starts(out, "[0-9]")]
    )
  }
  out
  
}

# Then display the table
tab %>% 
  filter(Country == "World") %>% 
  # Create a result string for table
  mutate(
    raw_result = str_glue(
      "{number2(raw_mean, pad = TRUE)}
      [{number2(raw_low)}, 
      {number2(raw_high)}] 
      ({percent2(pd)})"
    ),
    scaled_result = str_glue(
      "{number2(scaled_mean, pad = TRUE)} 
      [{number2(scaled_low)}, 
      {number2(scaled_high)}] 
      ({percent2(rope)})"
    ),
    # ROPE doesn't make sense for change over time (it is ROPE)
    scaled_result = if_else(Predictor == "Time", "", as.character(scaled_result))
  ) %>% 
  select(
    Predictor, 
    Outcome, 
    `Raw association` = raw_result, 
    `Scaled association` = scaled_result
  ) %>% 
  mutate(
    Predictor = as.character(Predictor),
    Predictor = if_else(Outcome == "Life satisfaction", Predictor, "")
  ) %>% 
  kable(escape = FALSE) %>% 
  kable_custom() %>% 
  column_spec(1, bold = TRUE) %>% 
  footnote(
    number = c(
      "Raw associations are percentage changes in outcomes associated with a decade (Time) or 10% (Internet & Mobile) increase in the predictor. Numbers indicate posterior means, [95%CI], and (posterior probability of direction).", 
      "Scaled associations are percentage changes in outcomes associated with per-decade change in the predictor (“decade-equivalent” association). Numbers indicate posterior means, [95%CI], and (posterior probability inside the region of practical equivalence; i.e. probability that the association is smaller than the per-decade change in the respective outcome)."
    )
  )
```

```{r include = FALSE}
# Are the sign tests and rope tests consistent?
tab %>% 
  count(d_nhst, d_rope)
tab %>% 
  filter(d_nhst == "Positive", d_rope == "Null")
country_post %>% 
  select(outcome, summary) %>% 
  unnest(summary) %>% 
  filter(country == "Bahrain", outcome == "Life_satisfaction")
```


## Heterogeneity

Next we look at the heterogeneity (deviations and correlations) of the parameters across countries, age groups, and their interactions.

### Variability

This table shows the standard deviations of the lower level parameters.

```{r}
#| label: tbl-variability
#| tbl-cap: "Heterogeneity in associations across countries, age groups, and their interaction."

get_het_pars <- function(outcome, predictor, model) {
  x <- readRDS(str_glue("models-small/brm-{outcome}-{model}.rds"))
  draws <- as_draws_df(
    x,
    variable = c(
      str_glue("^b_{predictor}$"),
      str_glue("^sd_age__{predictor}$"),
      str_glue("^sd_country__{predictor}$"),
      str_glue("^sd_age:country__{predictor}$")
    ),
    regex = TRUE
  ) %>% 
    select(-c(.chain, .iteration, .draw)) %>% 
    # From 100% to 10% units for associations
    mutate(across(-contains("val_year"), ~ ./10)) %>% 
    rename_with(~str_remove(., str_glue("{predictor}")))
  draws
}

get_het_pars <- memoise(get_het_pars, cache = cd)

het_pars <- fits %>% 
  crossing(
    nesting(predictor = c("val_year", "i1_cw", "m1_cw"), model = c(1, 3, 4))
  ) %>%
  mutate(
    out = pmap(
      list(outcome, predictor, model), 
      ~get_het_pars(..1, ..2, ..3))
  )
het_tab <- het_pars %>% 
  unnest(out) %>%
  select(-model) %>% 
  rename(
    Association = b_,
    `SD (age)` = sd_age__,
    `SD (country)` = sd_country__,
    `SD (age * country)` = `sd_age:country__`,
  ) %>%
  group_by(outcome, predictor) %>% 
  summarise_draws(mean, ~quantile2(., probs = c(.025, .975))) %>% 
  ungroup() %>% 
  mutate(
    across(c(mean, q2.5, q97.5), ~number(., .001)),
    res = as.character(str_glue("{mean} [{q2.5}, {q97.5}]")),
    Outcome = str_replace(outcome, "_", " "),
    Predictor = case_when(
      predictor == "val_year" ~ "Time",
      predictor == "i1_cw" ~ "Internet",
      predictor == "m1_cw" ~ "Mobile"
    ) %>% factor(levels = c("Time", "Internet", "Mobile"))
  ) %>% 
  select(
    Predictor, Outcome, variable, res
  ) %>% 
  mutate(res = if_else(res == "NA [NA, NA]", "", res)) %>% 
  pivot_wider(names_from = variable, values_from = res) %>% 
  mutate(Outcome = fct_inorder(Outcome)) %>% 
  select(Predictor, Outcome, Association, `SD (country)`, everything()) %>% 
  arrange(Predictor, Outcome)

het_tab %>% 
  select(-Association) %>% 
  mutate(
    Predictor = as.character(Predictor),
    Predictor = if_else(Outcome == "Life satisfaction", Predictor, "")
  ) %>% 
  kbl() %>% 
  kable_custom() %>% 
  column_spec(1, bold = TRUE) %>% 
  footnote(
    general = "Numbers indicate the standard deviations [95%CI] of the distributions of associations in the population of all countries.",
    general_title = "Note. ",
    footnote_as_chunk = TRUE
  )
```

### Correlations

These are the cross-country correlations of the well-being / mental health and internet / mobile broadband adoption intercepts and slopes.

```{r}
#| label: tbl-correlations
#| tbl-cap: "Correlations between country-level parameters"

get_cor_pars <- function(outcome, predictor, model) {
  x <- readRDS(str_glue("models-small/brm-{outcome}-{model}.rds"))
  as_draws_df(
    x,
    variable = c(
      str_glue("b_{predictor}_Intercept"),
      "b_val_Intercept",
      str_glue("b_{predictor}_year"),
      "b_val_year",
      str_glue("sd_country__{predictor}_Intercept"),
      "sd_country__val_Intercept",
      str_glue("sd_country__{predictor}_year"),
      "sd_country__val_year",
      str_glue("cor_country__{predictor}_year__val_year"),
      str_glue("cor_country__{predictor}_Intercept__val_Intercept")
    )
  )
}
get_cor_pars <- memoise(get_cor_pars, cache = cd)

cor_pars_brm <- fits %>%
  mutate(
    internet = map(outcome, ~get_cor_pars(.x, "internet", "1")),
    mobile = map(outcome, ~get_cor_pars(.x, "mobile", "2"))
  )

cor_sum <- cor_pars_brm %>% 
  mutate(
    across(
      internet:mobile, 
      ~map(., ~describe_posterior(., centrality = "mean", test = "pd"))
    )
  )
cor_sum %>% 
  pivot_longer(-outcome, names_to = "Predictor") %>% 
  unnest(value) %>% 
  filter(str_detect(Parameter, "cor_")) %>% 
  mutate(
    Mean = number2(Mean, .01, pad = TRUE),
    across(CI_low:CI_high, ~number(., .01)), 
    pd = percent2(pd, .1)
  ) %>% 
  transmute(
    Predictor = str_to_title(Predictor),
    Correlation = if_else(
      str_detect(Parameter, "Intercept"), "Intercepts", "Slopes"
    ),
    Outcome = str_replace(outcome, "_", " "),
    Summary = str_glue("{Mean} [{CI_low}, {CI_high}] ({pd})")
  ) %>% 
  pivot_wider(names_from = Predictor, values_from = Summary) %>% 
  mutate(Outcome = factor(Outcome, levels = str_replace(oo, "_", " "))) %>% 
  arrange(Correlation, Outcome) %>% 
  mutate(
    Correlation = if_else(Outcome == "Life satisfaction", Correlation, "")
  ) %>% 
  kbl(escape = FALSE) %>% 
  kable_custom() %>% 
  column_spec(1, bold = TRUE) %>% 
  footnote(
    general = "Numbers indicate the correlations [95%CI] between the intercepts and slopes of time of the corresponding outcome and internet / mobile broadband adoption.",
    general_title = "Note. ",
    footnote_as_chunk = TRUE
  )
```

## Countries

We then look at the country-level parameters. 

### Tables

This table shows the percentages of countries with credibly positive, null, and positive countries. 

```{r}
#| label: tbl-countries-sign
#| tbl-cap: "Country-level association classifications."

t1 <- tab %>% 
  filter(Country != "World") %>% 
  count(Predictor, Outcome, d_nhst) %>% 
  group_by(Outcome, Predictor) %>% 
  mutate(sum = sum(n), p = n / sum) %>% 
  transmute(
    Predictor,
    Outcome,
    Decision = d_nhst,
    p = percent(p, 1)
  ) %>% 
  pivot_wider(names_from = Decision, values_from = p) %>% 
  mutate(across(c(Negative, Null, Positive), ~replace_na(., "0%"))) 

t2 <- tab %>% 
  filter(Country != "World", Predictor != "Time") %>% 
  count(Predictor, Outcome, d_rope) %>% 
  group_by(Outcome, Predictor) %>% 
  mutate(sum = sum(n), p = n / sum) %>% 
  transmute(
    Predictor,
    Outcome,
    Decision = d_rope,
    p = percent(p, 1)
  ) %>% 
  pivot_wider(names_from = Decision, values_from = p) %>% 
  mutate(across(c(Negative, Null, Positive), ~replace_na(., "0%"))) 

left_join(t1, t2, by = c("Predictor", "Outcome"), suffix = c("", " ")) %>% 
  mutate(
    Predictor = as.character(Predictor),
    Predictor = if_else(Outcome == "Life satisfaction", Predictor, "")
  ) %>% 
  kbl() %>% 
  add_header_above(c(" " = 2, "Sign test" = 3, "ROPE test" = 4), align = "c") %>% 
  kable_custom() %>% 
  column_spec(1, bold = TRUE) %>% 
  footnote(
    general = "Numbers may not add up to 100% because of rounding.",
    general_title = "Note. ",
    footnote_as_chunk = TRUE
  )
```

All country-specific parameters are shown in [this table](country-table.html). There, `raw_` associations indicate associations in terms of decades (Time) or 10% change in predictor (Internet & Mobile). `scaled_` associations are decade-equivalent associations (association with how many % per decade the predictor changed). `pd` is probability of direction, and `rope` is posterior probability in ROPE as defined by the outcome's per-decade change over time. `d_` indicate decisions based on direction and ROPE.

```{r}
# Save interactive table to country-table.html
tab %>% 
  mutate(
    Country = relevel(factor(Country), ref = "World"),
    d_nhst = factor(d_nhst),
    d_rope = factor(d_rope)
  ) %>% 
  datatable(filter = "top", rownames = FALSE) %>% 
  formatRound(4:12, 3) %>% 
  saveWidget("country-table.html", title = "Country-specific parameters")
```

### Figure 2

For this one, we rescale the outcome~internet and outcome~mobile associations to be in units of decade-equivalent changes of internet and mobile.


```{r figure-2}
#| label: fig-2
#| fig-cap: "Country-level estimates"

# Arrange points within panels such that the order of countries is same in the rows of each column, running from smallest time change to greatest time change. This is a rudimentary way to show correlation between the two.
tmp <- tab %>% 
  filter(Predictor == "Time", Country != "World") %>% 
  arrange(raw_mean) %>% 
  group_by(Outcome) %>% 
  mutate(x_order = 1:n()) %>% 
  select(Outcome, Country, x_order) %>% 
  ungroup()

tab %>% 
  filter(Country != "World") %>% 
  left_join(tmp) %>% 
  # Factor order for colors
  mutate(
    color_nhst = recode(d_nhst, Null = "c", Negative = "b", Positive = "b")
  ) %>% 
  ggplot(aes(x_order, scaled_mean, col = color_nhst)) +
  scale_color_brewer(palette = "Paired", direction = -1) +
  # scale_color_manual(values = c("dodgerblue3", "dodgerblue1", "dodgerblue1")) +
  scale_x_continuous("Country") +
  scale_y_continuous(
    "Estimated linear association (%)"
  ) +
  geom_hline(yintercept = 0, lty = 2, size = .2) +
  geom_linerange(
    size = .15,
    aes(ymin = scaled_low, ymax = scaled_high)
  ) +
  geom_point(
    size = .2
  ) +
  facet_grid2(
    Predictor~Outcome, 
    scales = "free",
    independent = "all",
    labeller = labeller(
      .cols = as_labeller(~str_replace(., "_", " "))
    )
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    legend.position = "none"
  )
```

## Demographics

### Figure 3

```{r figure-3}
#| label: fig-3
#| fig-cap: "Demography-specific estimates"

summarise_demographics <- function(outcome) {
  
  fit1 <- readRDS(str_glue("models-small/brm-{outcome}-1.rds"))
  fit3 <- readRDS(str_glue("models-small/brm-{outcome}-3.rds"))
  fit4 <- readRDS(str_glue("models-small/brm-{outcome}-4.rds"))
  
  out_outcome_year <- spread_draws(
    fit1, 
    b_val_year, `b_val_year:sex1`, r_age__val[age, term] | term
  ) %>% 
    ungroup() %>% 
    mutate(
      World_Female = b_val_year + `b_val_year:sex1`*1,
      World_Male = b_val_year + `b_val_year:sex1`*-1,
      Age_Female = b_val_year + year + (`b_val_year:sex1` + `year:sex1`)*1,
      Age_Male = b_val_year + year + (`b_val_year:sex1` + `year:sex1`)*-1
    ) %>% 
    select(.draw, age, starts_with("World_"), starts_with("Age_")) %>% 
    pivot_wider(names_from = age, values_from = starts_with("Age_")) %>% 
    pivot_longer(-.draw, names_to = "Variable") %>% 
    # Throws a warning for Age cells in World rows (value doesn't exist)
    separate(Variable, into = c("Level", "Sex", "Age"), sep = "_") %>% 
    mutate(Age = str_replace_all(Age, "\\.", " "))
  
  out_outcome_internet <- spread_draws(
    fit3, 
    b_i1_cw, `b_i1_cw:sex1`, r_age[age, term] | term
  ) %>% 
    ungroup() %>% 
    mutate(
      World_Female = b_i1_cw + `b_i1_cw:sex1`*1,
      World_Male = b_i1_cw + `b_i1_cw:sex1`*-1,
      Age_Female = b_i1_cw + i1_cw + (`b_i1_cw:sex1` + `i1_cw:sex1`)*1,
      Age_Male = b_i1_cw + i1_cw + (`b_i1_cw:sex1` + `i1_cw:sex1`)*-1
    ) %>% 
    select(.draw, age, starts_with("World_"), starts_with("Age_")) %>% 
    pivot_wider(names_from = age, values_from = starts_with("Age_")) %>% 
    pivot_longer(-.draw, names_to = "Variable") %>% 
    separate(Variable, into = c("Level", "Sex", "Age"), sep = "_") %>% 
    mutate(Age = str_replace_all(Age, "\\.", " ")) %>% 
    # Rescale effect to "decade-equivalent"
    bind_cols(
      scaling_factors %>% 
        filter(y == "Internet", country == "World", outcome == {{outcome}}) %>% 
        select(scaling)
    ) %>% 
    mutate(value = value * scaling) %>% 
    select(-scaling)
  
  out_outcome_mobile <- spread_draws(
    fit4, 
    b_m1_cw, `b_m1_cw:sex1`, r_age[age, term] | term
  ) %>% 
    ungroup() %>% 
    mutate(
      World_Female = b_m1_cw + `b_m1_cw:sex1`*1,
      World_Male = b_m1_cw + `b_m1_cw:sex1`*-1,
      Age_Female = b_m1_cw + m1_cw + (`b_m1_cw:sex1` + `m1_cw:sex1`)*1,
      Age_Male = b_m1_cw + m1_cw + (`b_m1_cw:sex1` + `m1_cw:sex1`)*-1
    ) %>% 
    select(.draw, age, starts_with("World_"), starts_with("Age_")) %>% 
    pivot_wider(names_from = age, values_from = starts_with("Age_")) %>% 
    pivot_longer(-.draw, names_to = "Variable") %>% 
    separate(Variable, into = c("Level", "Sex", "Age"), sep = "_") %>% 
    mutate(Age = str_replace_all(Age, "\\.", " ")) %>% 
    # Rescale effect to "decade-equivalent"
    bind_cols(
      scaling_factors %>% 
        filter(y == "Internet", country == "World", outcome == {{outcome}}) %>% 
        select(scaling)
    ) %>% 
    mutate(value = value * scaling) %>% 
    select(-scaling)
  
  out <- bind_rows(
    "Time" = out_outcome_year,
    "Internet" = out_outcome_internet,
    "Mobile" = out_outcome_mobile,
    .id = "Predictor"
  ) %>% 
    mutate(Predictor = factor(Predictor, levels = c("Time", "Internet", "Mobile")))
  
  
  # Scaling factors are invariant to demography
  out <- scaling_factors %>% 
    filter(country == "World") %>% 
    filter(outcome == {{outcome}}) %>% 
    select(Predictor = y, scaling) %>% 
    right_join(out) %>% 
    mutate(scaling = if_else(Predictor == "Time", 1, scaling)) %>% 
    rename(raw = value) %>% 
    mutate(scaled = raw * scaling) %>% 
    select(-scaling)
  
  # Calculate demography-specific change-factors (how much outcome changed)
  change_factors <- out %>% 
    filter(Predictor == "Time") %>% 
    group_by(Level, Sex, Age) %>% 
    mean_qi(change = raw) %>% 
    select(Level, Sex, Age, change)
  
  # Then calculate pd and rope
  
  # Change associations with internet and mobile to units of 10% not 100%
  out <- out %>% 
    mutate(
      raw = if_else(Predictor == "Time", raw, raw / 10)
    )
  
  # Calculate the raw and scaled results for each outcome and predictor
  out <- out %>% 
    select(-.draw) %>% 
    nest(data = c(raw, scaled)) %>% 
    left_join(change_factors) %>% 
    mutate(
      out = map2(
        data, change,
        ~describe_posterior(
          .x, 
          centrality = "mean", 
          ci_method = "eti",
          rope_range = c(-.y, .y), 
          rope_ci = 1
        )
      )
    ) %>% 
    transmute(
      Predictor = factor(Predictor, levels = c("Time", "Internet", "Mobile")), 
      Level, Sex, Age, out
    ) %>% 
    arrange(Predictor) %>% 
    unnest(out) %>% 
    # Clean the names a bit
    rename(
      rope = ROPE_Percentage,
      mean = Mean,
      low = CI_low,
      high = CI_high
    ) %>% 
    # Put raw and scaled results on one row
    pivot_wider(
      names_from = Parameter,
      values_from = c(mean, low, high, pd, rope),
      names_glue = "{Parameter}_{.value}"
    ) %>% 
    # Leave symmetric ROPE limit in table
    mutate(rope_limit = abs(ROPE_low)) %>% 
    select(
      Predictor, 
      Level, Sex, Age,
      starts_with("raw_"), 
      starts_with("scaled_"),
      # pd is common to raw and scaled values, rope only applies to scaled values
      pd = raw_pd,
      rope = scaled_rope,
      rope_limit,
      -raw_rope, 
      -scaled_pd
    )
  
  # Calculate decision based on pd and rope
  out %>% 
    mutate(
      d_nhst = case_when(
        pd < 0.975 ~ "Null",
        pd >= 0.975 & sign(raw_mean) == -1 ~ "Negative",
        pd >= 0.975 & sign(raw_mean) == 1 ~ "Positive"
      ),
      d_rope = case_when(
        scaled_high < rope_limit*-1 ~ "Negative",
        scaled_low > rope_limit ~ "Positive",
        rope > 0.95 ~ "Null",
        TRUE ~ "Inconclusive"
      )
    )
}
summarise_demographics <- memoise(summarise_demographics, cache = cd)

out <- fits %>% 
  mutate(
    out = map(
      outcome,
      ~summarise_demographics(.x)
    )
  )

out %>% 
  unnest(out) %>% 
  # We'll use Age on the y axis, but also want to show the sex averages
  mutate(Age = if_else(is.na(Age), Sex, Age)) %>% 
  mutate(
    Age = factor(
      Age,
      levels = c(
        sort(levels(dat$age), decreasing = TRUE),
        "Male", "Female"
      )
    )
  ) %>%
  mutate(
    color_nhst = recode(d_nhst, Null = "a", Negative = "b", Positive = "b")
  ) %>% 
  ggplot(aes(scaled_mean, Age, col = color_nhst, shape = Sex)) +
  scale_color_brewer(palette = "Paired") +
  geom_vline(xintercept = 0, lty = 2, size = .2) +
  scale_x_continuous(
    "Estimated linear association (%)",
    breaks = extended_breaks(5),
    expand = expansion(.2)
  ) +
  geom_linerange(
    aes(xmin = scaled_low, xmax = scaled_high),
    size = .3,
    position = position_dodge2v(
      height = .7,
      padding = 1,
      reverse = TRUE
    )
  ) +
  geom_point(
    size = .9,
    position = position_dodge2v(
      height = .7,
      padding = 1,
      reverse = TRUE
    )
  ) +
  facet_grid(
    rows = vars(Predictor),
    cols = vars(outcome),
    scales = "free_x",
    labeller = as_labeller(~str_replace(., "_", " "))
  ) +
  theme(
    axis.title.y = element_blank(),
    legend.position = "none"
  )
```

